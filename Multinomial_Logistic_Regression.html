<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Mathematics Behind Multinomial Logistic Regression &mdash; attempt3 1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Workin with Links" href="Working_With_Links.html" />
    <link rel="prev" title="Mathematics behind Logistic Regression (Binomial)" href="2021_10_29_LogisticRegression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> attempt3
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="2021_10_17_PCA.html">Principal Component Anlysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="2021_10_29_LogisticRegression.html">Mathematics behind Logistic Regression (Binomial)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Mathematics Behind Multinomial Logistic Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Formulating-the-Problem">Formulating the Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Likelihood-and-Log-Likelihood">Likelihood and Log-Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Deriving-the-First-Derivative">Deriving the First Derivative</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Deriving-the-Hessian-Matrix">Deriving the Hessian Matrix</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Working_With_Links.html">Workin with Links</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">attempt3</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Mathematics Behind Multinomial Logistic Regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/Multinomial_Logistic_Regression.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Mathematics-Behind-Multinomial-Logistic-Regression">
<h1>Mathematics Behind Multinomial Logistic Regression<a class="headerlink" href="#Mathematics-Behind-Multinomial-Logistic-Regression" title="Permalink to this headline"></a></h1>
<section id="Formulating-the-Problem">
<h2>Formulating the Problem<a class="headerlink" href="#Formulating-the-Problem" title="Permalink to this headline"></a></h2>
<p>Suppose there are 3 classes in the response variable. Let us denote the classes by Class 1, Class 2 and Class 3.</p>
<p>We set,</p>
<div class="math notranslate nohighlight">
\[P(Y = 1|\vec{x}_i) = \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}, \ \
P(Y = 2|\vec{x}_i) = \frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\]</div>
<p>As a result,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
P(Y = 3|\vec{x}_i)
 &amp;= 1 \ - \ P(Y = 1|\vec{x}_i) - P(Y = 2|\vec{x}_i) \\
 \\
 &amp;= 1 - \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}-\frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
 \\
 &amp;= \frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}
\end{aligned}\end{split}\]</div>
</section>
<section id="Likelihood-and-Log-Likelihood">
<h2>Likelihood and Log-Likelihood<a class="headerlink" href="#Likelihood-and-Log-Likelihood" title="Permalink to this headline"></a></h2>
<p>So, likelihood,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L &amp;= \prod_{i=1}^n \left[P(y_i = 1 \ \ | \ \ \vec{x}_i)^{I(y_i=1)}\right].\left[P(y_i = 2 \ \ | \ \ \vec{x}_i)^{I(y_i=2)}\right].\left[P(y_i =  3 \ \ | \ \ \vec{x}_i)^{I(y_i=3)}\right] \\
 &amp;= \prod_{i=1}^n \left[\frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=1)}\left[\frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=2)}\left[\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=3)} \\
 &amp;= \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \ e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\left[\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{[I(y_i=1) + I(y_i=2) + I(y_i=3)]} \\
 &amp;= \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \ e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}
\end{aligned}\end{split}\]</div>
<p>Last equality holds as <span class="math notranslate nohighlight">\(I(y_i=1) + I(y_i=2) + I(y_i=3) = 1\)</span>.</p>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
log L &amp;= log \ \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \ e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
 &amp;= \sum_{i=1}^nlog\left[e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)}\right] + log\left[e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}\right] - log\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right] \\
 &amp;= \sum_{i=1}^n I(y_i=1).\vec{x}_i^T.\vec{\beta}_1 + \sum_{i=1}^nI(y_i=2).\vec{x}_i^T.\vec{\beta}_2 - \sum_{i=1}^nlog\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right]
\end{aligned}\end{split}\]</div>
</section>
<section id="Deriving-the-First-Derivative">
<h2>Deriving the First Derivative<a class="headerlink" href="#Deriving-the-First-Derivative" title="Permalink to this headline"></a></h2>
<p>First derivative,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\frac{\partial}{\partial \vec{\beta}}log L &amp;= \begin{pmatrix}\frac{\partial}{\partial \vec{\beta}_1}log L \\
\frac{\partial}{\partial \vec{\beta}_2}log L \end{pmatrix}
\end{aligned}\end{split}\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta_1}}log L &amp;= \frac{\partial}{\partial \vec{\beta_1}} \left[\sum_{i=1}^n I(y_i=1).\vec{x}_i^T.\vec{\beta}_1 + \sum_{i=1}^nI(y_i=2).\vec{x}_i^T.\vec{\beta}_2 - \sum_{i=1}^nlog\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right]\right] \\
 &amp;= \sum_{i=1}^n I(y_i=1).\vec{x}_i \ + \ 0  \ - \ \sum_{i=1}^n\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}.e^{\vec{x}_i^T.\vec{\beta}_1}.\vec{x}_i \\
 &amp;= \sum_{i=1}^n I(y_i=1) \ . \ \vec{x}_i \ \ - \ \ \sum_{i=1}^n P(y_i=1|\vec{x}_i) \ . \ \vec{x}_i \\
 &amp;= \sum_{i=1}^n \left[ \ \ I(y_i=1) - P(y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i
\end{aligned}\end{split}\]</div>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \vec{\beta_2}}log L = \sum_{i=1}^n \left[ \ \ I(y_i=2) - P(y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}}log L &amp;= \begin{pmatrix}\frac{\partial}{\partial \vec{\beta}_1}log L \\
\frac{\partial}{\partial \vec{\beta}_2}log L \end{pmatrix} \\
 \\
 &amp;= \begin{bmatrix}\sum_{i=1}^n \left[ \ \ I(y_i=1) - P(y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i \\
 \sum_{i=1}^n \left[ \ \ I(y_i=2) - P(y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\end{bmatrix} \\
 \\
 &amp;= \sum_{i=1}^n \begin{bmatrix} \left[ \ \ I(y_i=1) - P(y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i \\
 \left[ \ \ I(y_i=2) - P(y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\end{bmatrix} \\
 \\
 &amp;= \sum_{i=1}^n \begin{bmatrix}\ \ I(y_i=1) - P(y_i=1|\vec{x}_i) \ \  \\
 \ \ I(y_i=2) - P(y_i=2|\vec{x}_i) \ \ \end{bmatrix} \otimes \ \vec{x}_i
\end{aligned}\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\ \ \ \vec{\beta}^T = \left(\vec{\beta}_1^T \ \ \ \vec{\beta}_2^T\right)\)</span> and <span class="math notranslate nohighlight">\(\ \ \otimes\)</span> is <strong>Kronecker product</strong>.</p>
</section>
<section id="Deriving-the-Hessian-Matrix">
<h2>Deriving the Hessian Matrix<a class="headerlink" href="#Deriving-the-Hessian-Matrix" title="Permalink to this headline"></a></h2>
<p>Hessian matrix,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H &amp;= \frac{\partial}{\partial \vec{\beta}}\left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T \\
 &amp;= \begin{bmatrix}\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T \\
    \frac{\partial}{\partial \vec{\beta}_2} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T
    \end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T &amp;= \frac{\partial}{\partial \vec{\beta}_1} \sum_{i=1}^n\left[ \ I(y_i=1) - P(y_i=1|\vec{x}_i) \ \ \ \ I(y_i=2) - P(y_i=2|\vec{x}_i) \ \right] \otimes \vec{x}_i^T \\
 &amp;= \sum_{i=1}^n\left[ \ \frac{\partial}{\partial \vec{\beta}_1}\left[I(y_i=1) - P(y_i=1|\vec{x}_i)\right] \ \ \ \ \ \frac{\partial}{\partial \vec{\beta}_1}\left[I(y_i=2) - P(y_i=2|\vec{x}_i)\right] \ \right] \otimes \vec{x}_i^T \\
 &amp;= \sum_{i=1}^n \left[ \ \ 0 - \frac{\partial}{\partial \vec{\beta}_1}P(y_i=1|\vec{x}_i) \ \ \ \  0 - \frac{\partial}{\partial \vec{\beta}_1}P(y_i=2|\vec{x}_i) \ \ \right] \otimes \vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \left[ \ \ \frac{\partial}{\partial \vec{\beta}_1}P(y_i=1|\vec{x}_i) \ \ \ \  \frac{\partial}{\partial \vec{\beta}_1}P(y_i=2|\vec{x}_i) \ \ \right] \otimes \vec{x}_i^T
 \end{aligned}\end{split}\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \beta_{1j}} P(y_i=1|\vec{x}_i) &amp;= \frac{\partial}{\partial \beta_{1j}} \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
 \\
 &amp;= \frac{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}).e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij} - (e^{\vec{x}_i^T.\vec{\beta}_1})^2.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2} \\
 \\
 &amp;= \frac{e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2}.[1+e^{\vec{x}_i^T.\vec{\beta}_2}] \\
 \\
 &amp;= P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i).x_{ij}
\end{aligned}\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_1}P(y_i=1|\vec{x}_i) &amp;=
\begin{pmatrix}
\frac{\partial}{\partial \beta_{11}} P(y_i=1|\vec{x}_i) \\
\frac{\partial}{\partial \beta_{12}} P(y_i=1|\vec{x}_i) \\
. \\
. \\
. \\
\frac{\partial}{\partial \beta_{1p}} P(y_i=1|\vec{x}_i) \\
\end{pmatrix} \\
 \\
 &amp;= \begin{pmatrix}P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i).x_{i1} \\
 P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i).x_{i2} \\
 . \\
 . \\
 . \\
 P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i).x_{ip}
 \end{pmatrix} \\
 \\
 &amp;= P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i).\begin{pmatrix}x_{i1} \\
 x_{i2} \\
 x_{i3} \\
 . \\
 . \\
 . \\
 x_{ip}
 \end{pmatrix} \\
 \\
 &amp;= P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i).\vec{x}_i
\end{aligned}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[P(y_i=2|\vec{x}_i) = \frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\]</div>
<p>and,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \beta_{1j}} P(y_i=2|\vec{x}_i) &amp;= - \frac{e^{\vec{x}_i^T.\vec{\beta}_2}.e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2} \\
 &amp;= - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i).x_{ij} \\
\end{aligned}\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \vec{\beta}_1}P(y_i=2|\vec{x}_i) = - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i).\vec{x}_i\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_1}\left[\frac{\partial}{\partial \vec{\beta}}log L\right]^T &amp;= -\sum_{i=1}^n \left[ \ \ \frac{\partial}{\partial \vec{\beta}_1}P(y_i=1|\vec{x}_i) \ \ \ \  \frac{\partial}{\partial \vec{\beta}_1}P(y_i=2|\vec{x}_i) \ \ \right] \otimes \vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \left[P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i).\vec{x}_i \ \ \ \  - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i).\vec{x}_i\right] \otimes \vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n\left[P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i) \ \ \ \ - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i\otimes\vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n\left[P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i) \ \ \ \ - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{aligned}\end{split}\]</div>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_2}\left[\frac{\partial}{\partial \vec{\beta}}log L\right]^T = -\sum_{i=1}^n\left[-P(y_i=2|\vec{x}_i)P(y_i=1|\vec{x}_i) \ \ \ \ P(y_i=2|\vec{x}_i)P(y_i\neq2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{aligned}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 H &amp;= \begin{bmatrix}\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T \\
    \frac{\partial}{\partial \vec{\beta}_2} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T
    \end{bmatrix} \\
  \\
  &amp;= \begin{bmatrix}-\sum_{i=1}^n\left[P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i) \ \ \ \ \ \ - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T \\
  -\sum_{i=1}^n\left[-P(y_i=2|\vec{x}_i).P(y_i=1|\vec{x}_i) \ \ \ \ \ \ \ P(y_i=2|\vec{x}_i).P(y_i\neq2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{bmatrix} \\
  \\
  &amp;= -\sum_{i=1}^n\begin{bmatrix}P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i) \\
  -P(y_i=2|\vec{x}_i).P(y_i=1|\vec{x}_i) \ \ \ \ \ \ P(y_i=2|\vec{x}_i).P(y_i\neq2|\vec{x}_i)
  \end{bmatrix} \otimes \vec{x}_i.\vec{x}_i^T
 \end{aligned}\end{split}\]</div>
<p>To have unique maxima to the problem, <span class="math notranslate nohighlight">\(H\)</span> has to be negative semi-definite, or <span class="math notranslate nohighlight">\(-H\)</span> positive semi-definite. We can prove it in the following way. 1. Sum of positive definite matrices is positive definite. -H is sum of matrices. So, if we can show that the term inside sum is positive definite then -H is positive definite. 2. Kronecker Product of two positive semi-definite matrices is positive semi-definite. It follows from the following theory:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>**Theory:** Let, $\mathbf{A_{mxm}}$ has eigen vectors **$\mathbf{x_i}$** with corresponding eigen value $\mathbf{\lambda_i, i = 1, \dots, m}$;
$\mathbf{B_{nxn}}$ has eighen vectors $\mathbf{y_j}$ with corresponding eigen value $\mathbf{\mu_j, j = 1, \dots n}$.
Then, eigen vectors of $\mathbf{A\otimes B}$ is $\mathbf{x_i \otimes y_j}$ with corresponding eigen value $\mathbf{\lambda_i.\mu_j}$.

**Proof:** Here, $Ax_i = \lambda_i x_i$ and $By_j = \mu_j y_j$. Now by the property (standard easily available on internet) of Kronecker Product $(A\otimes B)(x_i\otimes y_j) = (Ax_i)\otimes (By_j)=(\lambda_i x_i)\otimes (\mu_j y_j) = \lambda_i\mu_j(x_i\otimes y_j)$. So, $x_i\otimes y_j$ is the eigen vector corresponding to eigenvector $\lambda_i\mu_j$

Now, if **A** and **B** pd then both $\lambda_i$ and $\mu_j$ are non-negative. So is $\lambda_i\mu_j$, showing $A\otimes B$ is positive semi-definite.

So if we can show that both the operands of the Kronecker Product is positve semi-definite, then -H will be postive semi-definite.
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p><span class="math notranslate nohighlight">\(\vec{x}_i.\vec{x}_i^T\)</span> is positive semi-definite. We have to prove <span class="math notranslate nohighlight">\(\begin{bmatrix}P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i) \\  -P(y_i=2|\vec{x}_i).P(y_i=1|\vec{x}_i) \ \ \ \ \ \ P(y_i=2|\vec{x}_i).P(y_i\neq2|\vec{x}_i)  \end{bmatrix}\)</span> is positive semi-definite.</p></li>
</ol>
<p>Now let,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
   P &amp;= \begin{bmatrix}
   P(y_i=1|\vec{x}_i).P(y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i) \\
  -P(y_i=2|\vec{x}_i).P(y_i=1|\vec{x}_i) \ \ \ \ \ \ P(y_i=2|\vec{x}_i).P(y_i\neq2|\vec{x}_i)
  \end{bmatrix} \\
   \\
   &amp;= \begin{bmatrix}
   P(y_i=1|\vec{x}_i).[1-P(y_i=1|\vec{x}_i)] &amp; - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i) \\
  -P(y_i=2|\vec{x}_i).P(y_i=1|\vec{x}_i) &amp; P(y_i=2|\vec{x}_i).[1-P(y_i=2|\vec{x}_i)]
  \end{bmatrix} \\
   \\
   &amp;= \begin{bmatrix}
   P(y_i=1|\vec{x}_i)-P^2(y_i=1|\vec{x}_i) &amp; - P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i) \\
  -P(y_i=2|\vec{x}_i).P(y_i=1|\vec{x}_i) &amp; P(y_i=2|\vec{x}_i)-P^2(y_i=2|\vec{x}_i)]
  \end{bmatrix} \\
   \\
   &amp;= \begin{bmatrix}
   P(y_i=1|\vec{x}_i) &amp; 0 \\
   0 &amp; P(y_i=2|\vec{x}_i)
   \end{bmatrix} -
   \begin{bmatrix}
   P^2(y_i=1|\vec{x}_i) &amp; P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i) \\
   P(y_i=1|\vec{x}_i).P(y_i=2|\vec{x}_i) &amp; P^2(y_i=2|\vec{x}_i)\end{bmatrix} \\
   \\
   &amp;= \begin{bmatrix}
   P(y_i=1|\vec{x}_i) &amp; 0 \\
   0 &amp; P(y_i=2|\vec{x}_i)
   \end{bmatrix} -
   \begin{bmatrix}
   P(y_i=1|\vec{x}_i) \\
   P(y_i=2|\vec{x}_i)\end{bmatrix}.
   \begin{bmatrix}
   P(y_i=1|\vec{x}_i) \\
   P(y_i=2|\vec{x}_i)\end{bmatrix}^T
   \end{aligned}\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
   &amp;  \vec{z}^T.P.\vec{z} \\
   &amp;= \vec{z}^T.\begin{bmatrix}
   P(y_i=1|\vec{x}_i) &amp; 0 \\
   0 &amp; P(y_i=2|\vec{x}_i)
   \end{bmatrix}.\vec{z} - \vec{z}^T.
   \begin{bmatrix}
   P(y_i=1|\vec{x}_i) \\
   P(y_i=2|\vec{x}_i)\end{bmatrix}.
   \begin{bmatrix}
   P(y_i=1|\vec{x}_i) \\
   P(y_i=2|\vec{x}_i)\end{bmatrix}^T.\vec{z} \\
   \\
   &amp;= \sum_{j=1}^2P(y_i=j|\vec{x}_i)z_j^2 \ \ - \ \ \left(\sum_{j=1}^2P(y_i=j|\vec{x}_i)z_j\right)^2  \end{aligned}\end{split}\]</div>
<p>Now, by <strong>Cauchy-Schwarz Inequality</strong>,</p>
<div class="math notranslate nohighlight">
\[\left(\sum_{j=1}^n u_jv_j\right)^2 \ \ \leq \ \ \sum_{j=1}^n u_j^2.\sum_{j=1}^nv_j^2\]</div>
<p>If we set, n =2, <span class="math notranslate nohighlight">\(u_j=\sqrt{P(y_i=j|\vec{x}_i)}\)</span> also as, <span class="math notranslate nohighlight">\(P(y_i=j|\vec{x}_i) \geq 0\)</span>, we can set <span class="math notranslate nohighlight">\(v_j=\sqrt{P(y_i=j|\vec{x}_i)}.z_j\)</span>; we get,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 &amp;\left(\sum_{j=1}^2 \sqrt{P(y_i=j|\vec{x}_i)}.\sqrt{P(y_i=j|\vec{x}_i)}.z_j\right)^2 \leq \sum_{j=1}^2P(y_i=j|\vec{x}_i).\sum_{j=1}^2P(y_i=j|\vec{x}_i).z_j^2 \\
 \implies &amp; \left(\sum_{j=1}^2 P(y_i=j|\vec{x}_i).z_j\right)^2 \leq \sum_{j=1}^2P(y_i=j|\vec{x}_i).z_j^2
\end{aligned}\end{split}\]</div>
<p>As, <span class="math notranslate nohighlight">\(\sum_{j=1}^2P(y_i=j|\vec{x}_i) = 1\)</span>.</p>
<p>So, <span class="math notranslate nohighlight">\(\vec{z}^T.P.\vec{z} \geq 0\)</span>. So, P is positive semi-definite.</p>
<p>Using the above steps we can prove that -H is negative semi-definite, so H is positive semi-definite.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2021_10_29_LogisticRegression.html" class="btn btn-neutral float-left" title="Mathematics behind Logistic Regression (Binomial)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Working_With_Links.html" class="btn btn-neutral float-right" title="Workin with Links" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Kaustav.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>