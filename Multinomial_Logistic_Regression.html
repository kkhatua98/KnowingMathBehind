<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Mathematics Behind Multinomial Logistic Regression &mdash; attempt3 1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Workin with Links" href="Working_With_Links.html" />
    <link rel="prev" title="Mathematics behind Logistic Regression (Binomial)" href="2021_10_29_LogisticRegression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> attempt3
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="2021_10_17_PCA.html">Principal Component Anlysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="2021_10_29_LogisticRegression.html">Mathematics behind Logistic Regression (Binomial)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Mathematics Behind Multinomial Logistic Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Formulating-the-Problem">Formulating the Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Parameter-Estimation-using-MLE">Parameter Estimation using MLE</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Deriving-the-Log-Likelihood">Deriving the Log-Likelihood</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Deriving-the-First-Derivative">Deriving the First Derivative</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Estimating-the-Parameters">Estimating the Parameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Proving-Concavity-of-the-Problem">Proving Concavity of the Problem</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Deriving-Hessian-Matrix">Deriving Hessian Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="#Proving-Concavity-of-Hessian-Matrix">Proving Concavity of Hessian Matrix</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Working_With_Links.html">Workin with Links</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">attempt3</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Mathematics Behind Multinomial Logistic Regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/Multinomial_Logistic_Regression.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Mathematics-Behind-Multinomial-Logistic-Regression">
<h1>Mathematics Behind Multinomial Logistic Regression<a class="headerlink" href="#Mathematics-Behind-Multinomial-Logistic-Regression" title="Permalink to this headline"></a></h1>
<section id="Formulating-the-Problem">
<h2>Formulating the Problem<a class="headerlink" href="#Formulating-the-Problem" title="Permalink to this headline"></a></h2>
<p>Suppose, a p dimensional vector <span class="math notranslate nohighlight">\(x\)</span> is our explanatory variable and <span class="math notranslate nohighlight">\(Y\)</span> is the dependent variable. Now unlike Binomial logistic regression where there were two independent classes in <span class="math notranslate nohighlight">\(Y\)</span>, here the number of classes is more than 2. For better explanation without doing much calculation we are assuming that, there are 3 classes in the response variable <span class="math notranslate nohighlight">\(Y\)</span>. Let us denote the classes by <span class="math notranslate nohighlight">\(Y\)</span> = 1, <span class="math notranslate nohighlight">\(Y\)</span> = 2 and <span class="math notranslate nohighlight">\(Y\)</span> = 3.</p>
<p>We assume that <span class="math notranslate nohighlight">\(Y\)</span> follows a multinomial distribution with three classes that has probability function,</p>
<div class="math notranslate nohighlight">
\[P(Y = y|\vec{x}) = p_1^{I(y=1)}.p_2^{I(y=2)}.p_3^{I(y=3)} \ \ \ \ \ \ ........ (1)\]</div>
<p>where, the first two conditional probabilities are as follows,</p>
<div class="math notranslate nohighlight">
\[p_1 = P(Y = 1|\vec{x}) = \frac{e^{\vec{x}^T.\vec{\beta}_1}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}, \ \
p_2 = P(Y = 2|\vec{x}) = \frac{e^{\vec{x}^T.\vec{\beta}_2}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}\]</div>
<p>As sum of the conditional probabilities for the three classes is 1 so,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
p_3 = P(Y = 3|\vec{x})
 &amp;= 1 \ - \ P(Y = 1|\vec{x}) - P(Y = 2|\vec{x}) \\
 \\
 &amp;= 1 - \frac{e^{\vec{x}^T.\vec{\beta}_1}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}-\frac{e^{\vec{x}^T.\vec{\beta}_2}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}} \\
 \\
 &amp;= \frac{1}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}
\end{aligned}\end{split}\]</div>
<p>In the above three probabilities <span class="math notranslate nohighlight">\(\vec{\beta}_1\)</span> and <span class="math notranslate nohighlight">\(\vec{\beta}_2\)</span> are p dimensional unknown vectors and we have to estimate them using some parameter estimation technic.</p>
</section>
<section id="Parameter-Estimation-using-MLE">
<h2>Parameter Estimation using MLE<a class="headerlink" href="#Parameter-Estimation-using-MLE" title="Permalink to this headline"></a></h2>
<p>In MLE we first calculate likelihood take log of it, and calculate the set of parameters for which the log-likelihood will be maximum. In simple terms, we will build a function using the unknown parameters, that function will be called likelihood, we take log of it and maximize it with respect to the parameters.</p>
</section>
</section>
<section id="Deriving-the-Log-Likelihood">
<h1>Deriving the Log-Likelihood<a class="headerlink" href="#Deriving-the-Log-Likelihood" title="Permalink to this headline"></a></h1>
<p>Suppose there are n number of observations in the data; and for each of the observations <span class="math notranslate nohighlight">\((\vec{x}_i, y_i)\)</span>, <span class="math notranslate nohighlight">\(\vec{x}_i\)</span> is the value of the regressor vector and <span class="math notranslate nohighlight">\(y_i\)</span> is the observed value of the response variable. Think like, each of the <span class="math notranslate nohighlight">\(y_i\)</span>s is an observed value of a random variable with before mentioned multinomial distribution, ie. each of the <span class="math notranslate nohighlight">\(y_i\)</span>s come from a <span class="math notranslate nohighlight">\(Y_i\)</span> where distribution of <span class="math notranslate nohighlight">\(Y_i\)</span> is,
<span class="math notranslate nohighlight">\(P(Y_i = y_i|\vec{x}_i) = p_{1i}^{I(y_i=1)}.p_{2i}^{I(y_i=2)}.p_{3i}^{I(y_i=3)}\)</span> where,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;p_{1i} = \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
&amp;p_{2i} = \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
&amp;p_{3i} = \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}
\end{aligned}\end{split}\]</div>
<p>Now, if you get confused from where the i comes, you can think that, for each of the n observations value of <span class="math notranslate nohighlight">\(x\)</span> is different and we are denoting the different values by <span class="math notranslate nohighlight">\(x_i\)</span>; now as <span class="math notranslate nohighlight">\(P(Y = y|\vec{x})\)</span> is a conditional distribution on <span class="math notranslate nohighlight">\(x\)</span>, so if <span class="math notranslate nohighlight">\(x\)</span> changes (<span class="math notranslate nohighlight">\(x_1, x_2, x_3, ..., x_n\)</span>) the conditional distribution changes.</p>
<p>So, likelihood,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L &amp;= \prod_{i=1}^n P(Y_i = y_i|\vec{x}_i) \\
 &amp;= \prod_{i=1}^n p_{1i}^{I(y_i=1)}.p_{2i}^{I(y_i=2)}.p_{3i}^{I(y_i=3)} \\
 &amp;= \prod_{i=1}^n \left[\frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=1)}\left[\frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=2)}\left[\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=3)} \\
 &amp;= \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \ e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\left[\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{[I(y_i=1) + I(y_i=2) + I(y_i=3)]} \\
 &amp;= \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \ e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}
\end{aligned}\end{split}\]</div>
<p>Last equality holds as <span class="math notranslate nohighlight">\(I(y_i=1) + I(y_i=2) + I(y_i=3) = 1\)</span>.</p>
<p>So log-likelihood,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
log L &amp;= log \ \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \ e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
 &amp;= \sum_{i=1}^nlog\left[e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)}\right] + log\left[e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}\right] - log\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right] \\
 &amp;= \sum_{i=1}^n I(y_i=1).\vec{x}_i^T.\vec{\beta}_1 + \sum_{i=1}^nI(y_i=2).\vec{x}_i^T.\vec{\beta}_2 - \sum_{i=1}^nlog\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right]
\end{aligned}\end{split}\]</div>
</section>
<section id="Deriving-the-First-Derivative">
<h1>Deriving the First Derivative<a class="headerlink" href="#Deriving-the-First-Derivative" title="Permalink to this headline"></a></h1>
<p>First derivative,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\frac{\partial}{\partial \vec{\beta}}log L &amp;= \begin{pmatrix}\frac{\partial}{\partial \vec{\beta}_1}log L \\
\frac{\partial}{\partial \vec{\beta}_2}log L \end{pmatrix}
\end{aligned}\end{split}\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta_1}}log L &amp;= \frac{\partial}{\partial \vec{\beta_1}} \left[\sum_{i=1}^n I(y_i=1).\vec{x}_i^T.\vec{\beta}_1 + \sum_{i=1}^nI(y_i=2).\vec{x}_i^T.\vec{\beta}_2 - \sum_{i=1}^nlog\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right]\right] \\
 &amp;= \sum_{i=1}^n I(y_i=1).\vec{x}_i \ + \ 0  \ - \ \sum_{i=1}^n\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}.e^{\vec{x}_i^T.\vec{\beta}_1}.\vec{x}_i \\
 &amp;= \sum_{i=1}^n I(y_i=1) \ . \ \vec{x}_i \ \ - \ \ \sum_{i=1}^n P(Y_i=1|\vec{x}_i) \ . \ \vec{x}_i \\
 &amp;= \sum_{i=1}^n \left[ \ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i
\end{aligned}\end{split}\]</div>
<p>In the second line the second term is independent of <span class="math notranslate nohighlight">\(\beta_1\)</span>, so derivative of it with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span> is 0.</p>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \vec{\beta_2}}log L = \sum_{i=1}^n \left[ \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}}log L &amp;= \begin{pmatrix}\frac{\partial}{\partial \vec{\beta}_1}log L \\
\frac{\partial}{\partial \vec{\beta}_2}log L \end{pmatrix} \\
 \\
 &amp;= \begin{bmatrix}\sum_{i=1}^n \left[ \ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i \\
 \sum_{i=1}^n \left[ \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\end{bmatrix} \\
 \\
 &amp;= \sum_{i=1}^n \begin{bmatrix} \left[ \ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i \\
 \left[ \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\end{bmatrix} \\
 \\
 &amp;= \sum_{i=1}^n \begin{bmatrix}\ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \  \\
 \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \end{bmatrix} \otimes \ \vec{x}_i
\end{aligned}\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\ \ \ \vec{\beta}^T = \left(\vec{\beta}_1^T \ \ \ \vec{\beta}_2^T\right)\)</span> and <span class="math notranslate nohighlight">\(\ \ \otimes\)</span> is <strong>Kronecker product</strong>. Note that, the unknown parameters are hidden inside <span class="math notranslate nohighlight">\(P(Y_i=1|\vec{x}_i)\)</span> and <span class="math notranslate nohighlight">\(P(Y_i=2|\vec{x}_i)\)</span>.</p>
</section>
<section id="Estimating-the-Parameters">
<h1>Estimating the Parameters<a class="headerlink" href="#Estimating-the-Parameters" title="Permalink to this headline"></a></h1>
<p>Now to get the parameter estimates, we will solve the equation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial \vec{\beta}}log L=\sum_{i=1}^n \begin{bmatrix}\ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \  \\
 \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \end{bmatrix} \otimes \ \vec{x}_i = 0\end{split}\]</div>
<p>But here the problem is that, due to the complicated form of <span class="math notranslate nohighlight">\(P(Y_i=1|\vec{x}_i)\)</span> and <span class="math notranslate nohighlight">\(P(Y_i=2|\vec{x}_i)\)</span> we can not solve the set equations as linear regression and get a closed form of the parameter estimates, to get the parameter estimates we have to use numerical methods. But we will not discuss it here, we will see various numerical methods first and come back to it again.</p>
<section id="Proving-Concavity-of-the-Problem">
<h2>Proving Concavity of the Problem<a class="headerlink" href="#Proving-Concavity-of-the-Problem" title="Permalink to this headline"></a></h2>
<p>Now we are in a state that we have a function and we are maximizing it, but it is very common in maximization and minimization problem that the equation <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \vec{\beta}}log L = 0\)</span> yields more than one solution, so which one to take, in that case we have to calculate value of the log-likelihood for each of the solution and select the one for which log-likelihood is maximum. But this is not the case with concave functions, concave functions have unique solution to
<span class="math notranslate nohighlight">\(\frac{\partial}{\partial \vec{\beta}}log L = 0\)</span> and this is the point of maxima. Now the necessary and sufficient condition for a function to be concave is the <strong>Hessian matrix</strong> of the function is negative semi-definite. So if we can prove that Hessian matrix of our problem is negative semi-definite we can be sure that the solution of the above equation will give the point of maxima.</p>
</section>
</section>
<section id="Deriving-Hessian-Matrix">
<h1>Deriving Hessian Matrix<a class="headerlink" href="#Deriving-Hessian-Matrix" title="Permalink to this headline"></a></h1>
<p>Hessian matrix,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H &amp;= \frac{\partial}{\partial \vec{\beta}}\left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T \\
 &amp;= \begin{bmatrix}\underbrace{\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T}_{H_1} \\
    \underbrace{\frac{\partial}{\partial \vec{\beta}_2} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T}_{H_2}
    \end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>Calculating the derivatives in the hessian matrix is not staright forward. So, we will derive them seperately.</p>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T &amp;= \frac{\partial}{\partial \vec{\beta}_1} \sum_{i=1}^n\left[ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \right] \otimes \vec{x}_i^T \\
 &amp;= \sum_{i=1}^n\left[ \ \frac{\partial}{\partial \vec{\beta}_1}\left[I(y_i=1) - P(Y_i=1|\vec{x}_i)\right] \ \ \ \ \ \frac{\partial}{\partial \vec{\beta}_1}\left[I(y_i=2) - P(Y_i=2|\vec{x}_i)\right] \ \right] \otimes \vec{x}_i^T \\
 &amp;= \sum_{i=1}^n \left[ \ \ 0 - \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i) \ \ \ \  0 - \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i) \ \ \right] \otimes \vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \left[ \ \ \underbrace{\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i)}_* \ \ \ \  \underbrace{\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i)}_{**} \ \ \right] \otimes \vec{x}_i^T &amp;(1)
\end{aligned}\end{split}\]</div>
<p>Again two derivatives, will do it seperately.</p>
<p>Deriving * It is always recommended to use vector matrix differentiation where differentiation with respect to a vector is involved, alternatively we can calculate derivatives with respect to every element of <span class="math notranslate nohighlight">\(\beta_1\)</span> (ie. <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \beta_{11}}P(Y_i=1|\vec{x}_i)\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \beta_{12}}P(Y_i=1|\vec{x}_i)\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \beta_{13}}P(Y_i=1|\vec{x}_i)\)</span>, …,<span class="math notranslate nohighlight">\(\frac{\partial}{\partial \beta_{1p}}P(Y_i=1|\vec{x}_i)\)</span>) and stack them one below other to get <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \beta_{1j}} P(Y_i=1|\vec{x}_i) &amp;= \frac{\partial}{\partial \beta_{1j}} \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
 \\
 &amp;= \frac{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}).e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij} - (e^{\vec{x}_i^T.\vec{\beta}_1})^2.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2} \\
 \\
 &amp;= \frac{e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2}.[1+e^{\vec{x}_i^T.\vec{\beta}_2}] \\
 \\
 &amp;= P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{ij}
\end{aligned}\end{split}\]</div>
<hr class="docutils" />
<p>So, we know the form of each of the derivative, now we can vary j and stack the elements one below other.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i) &amp;=
\begin{pmatrix}
\frac{\partial}{\partial \beta_{11}} P(Y_i=1|\vec{x}_i) \\
\frac{\partial}{\partial \beta_{12}} P(Y_i=1|\vec{x}_i) \\
. \\
. \\
. \\
\frac{\partial}{\partial \beta_{1p}} P(Y_i=1|\vec{x}_i) \\
\end{pmatrix} \\
 \\
 &amp;= \begin{pmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{i1} \\
 P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{i2} \\
 . \\
 . \\
 . \\
 P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{ip}
 \end{pmatrix} \\
 \\
 &amp;= P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).\begin{pmatrix}x_{i1} \\
 x_{i2} \\
 x_{i3} \\
 . \\
 . \\
 . \\
 x_{ip}
 \end{pmatrix} \\
 \\
 &amp;= P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).\vec{x}_i
\end{aligned}\end{split}\]</div>
<p>Deriving **</p>
<div class="math notranslate nohighlight">
\[P(Y_i=2|\vec{x}_i) = \frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\]</div>
<hr class="docutils" />
<p>and,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \beta_{1j}} P(Y_i=2|\vec{x}_i) &amp;= - \frac{e^{\vec{x}_i^T.\vec{\beta}_2}.e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2} \\
 &amp;= - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i).x_{ij} \\
\end{aligned}\end{split}\]</div>
<p>So again if we stack the partial derivatives one below other as before, we will get the required partial derivative vector.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i) = - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i).\vec{x}_i\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_1}\left[\frac{\partial}{\partial \vec{\beta}}log L\right]^T &amp;= -\sum_{i=1}^n \left[ \ \ \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i) \ \ \ \  \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i) \ \ \right] \otimes \vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).\vec{x}_i \ \ \ \  - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i).\vec{x}_i\right] \otimes \vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n\left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i\otimes\vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n\left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{aligned}\end{split}\]</div>
<p>Similarly if we calculate the derivatives properly we can get,</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\frac{\partial}{\partial \vec{\beta}_2}\left[\frac{\partial}{\partial \vec{\beta}}log L\right]^T = -\sum_{i=1}^n\left[-P(Y_i=2|\vec{x}_i)P(Y_i=1|\vec{x}_i) \ \ \ \ P(Y_i=2|\vec{x}_i)P(Y_i\neq2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{aligned}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 H &amp;= \begin{bmatrix}\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T \\
    \frac{\partial}{\partial \vec{\beta}_2} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T
    \end{bmatrix} \\
  \\
  &amp;= \begin{bmatrix}-\sum_{i=1}^n\left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T \\
  -\sum_{i=1}^n\left[-P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{bmatrix} \\
  \\
  &amp;= -\sum_{i=1}^n\begin{bmatrix}P(y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
  -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)
  \end{bmatrix} \otimes \vec{x}_i.\vec{x}_i^T
 \end{aligned}\end{split}\]</div>
</section>
<section id="Proving-Concavity-of-Hessian-Matrix">
<h1>Proving Concavity of Hessian Matrix<a class="headerlink" href="#Proving-Concavity-of-Hessian-Matrix" title="Permalink to this headline"></a></h1>
<p>To have unique maxima to the problem, <span class="math notranslate nohighlight">\(H\)</span> has to be negative semi-definite, or <span class="math notranslate nohighlight">\(-H\)</span> positive semi-definite. We can prove it using the following theorems. ### 1. Sum of positive semi-definite matrices is positive semi-definite. -H is sum of matrices. So, if we can show that the term inside sum is positive semi-definite then -H is positive semi-definite; ie. we have to prove
<span class="math notranslate nohighlight">\(S_i=\begin{bmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\ -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)\end{bmatrix} \otimes \vec{x}_i.\vec{x}_i^T\)</span> is positive semi-definite for each of <span class="math notranslate nohighlight">\(i\)</span>. It can be proved using the following theorem.</p>
<p>So the problem boils down to showing positive semi-definniteness of <span class="math notranslate nohighlight">\(\begin{bmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\ -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i) \end{bmatrix}\)</span> and <span class="math notranslate nohighlight">\(\vec{x}_i.\vec{x}_i^T\)</span>. It follows from the following theory:</p>
<p><strong>Proof:</strong> To prove it we have to show, for any <span class="math notranslate nohighlight">\(z\neq0\)</span> vector, <span class="math notranslate nohighlight">\(\vec{z}^T.\vec{v}.\vec{v}^T.\vec{z}\geq\)</span>. Now, <span class="math notranslate nohighlight">\(\vec{z}^T.\vec{v}.\vec{v}^T.\vec{z}=\left(\vec{z}^T.\vec{v}\right)^2\geq0\)</span>, because <span class="math notranslate nohighlight">\(\vec{z}^T.\vec{v}=\vec{v}^T.\vec{z}=\)</span> a scalar. If we replace <span class="math notranslate nohighlight">\(v\)</span> with <span class="math notranslate nohighlight">\(\vec{x}_i\)</span> we will get<span class="math notranslate nohighlight">\(\vec{x}_i.\vec{x}_i^T\)</span> is positive semi-definite.</p>
<p>Only thing remaining to prove, positive semi-definiteness of <span class="math notranslate nohighlight">\(\begin{bmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\ -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i) \end{bmatrix}\)</span>.</p>
<p>Now let,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
P &amp;= \begin{bmatrix}
P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
-P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)
\end{bmatrix} \\
\\
&amp;= \begin{bmatrix}
P(Y_i=1|\vec{x}_i).[1-P(Y_i=1|\vec{x}_i)] &amp; - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
-P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) &amp; P(Y_i=2|\vec{x}_i).[1-P(Y_i=2|\vec{x}_i)]
\end{bmatrix} \\
\\
&amp;= \begin{bmatrix}
P(Y_i=1|\vec{x}_i)-P^2(Y_i=1|\vec{x}_i) &amp; - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
-P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) &amp; P(Y_i=2|\vec{x}_i)-P^2(Y_i=2|\vec{x}_i)]
\end{bmatrix} \\
\\
&amp;= \begin{bmatrix}
P(Y_i=1|\vec{x}_i) &amp; 0 \\
0 &amp; P(Y_i=2|\vec{x}_i)
\end{bmatrix} -
\begin{bmatrix}
P^2(Y_i=1|\vec{x}_i) &amp; P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) &amp; P^2(Y_i=2|\vec{x}_i)\end{bmatrix} \\
\\
&amp;= \begin{bmatrix}
P(Y_i=1|\vec{x}_i) &amp; 0 \\
0 &amp; P(Y_i=2|\vec{x}_i)
\end{bmatrix} -
\begin{bmatrix}
P(Y_i=1|\vec{x}_i) \\
P(Y_i=2|\vec{x}_i)\end{bmatrix}.
\begin{bmatrix}
P(Y_i=1|\vec{x}_i) \\
P(Y_i=2|\vec{x}_i)\end{bmatrix}^T
\end{aligned}\end{split}\]</div>
<p>Now, to prove the positive semi-definiteness, we have to show for any <span class="math notranslate nohighlight">\(z\neq0\)</span>, <span class="math notranslate nohighlight">\(\vec{z}^T.P.\vec{z}\geq0\)</span>. So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;  \vec{z}^T.P.\vec{z} \\
&amp;= \vec{z}^T.\begin{bmatrix}
P(Y_i=1|\vec{x}_i) &amp; 0 \\
0 &amp; P(Y_i=2|\vec{x}_i)
\end{bmatrix}.\vec{z} - \vec{z}^T.
\begin{bmatrix}
P(Y_i=1|\vec{x}_i) \\
P(Y_i=2|\vec{x}_i)\end{bmatrix}.
\begin{bmatrix}
P(Y_i=1|\vec{x}_i) \\
P(Y_i=2|\vec{x}_i)\end{bmatrix}^T.\vec{z} \\
\\
&amp;= \sum_{j=1}^2P(Y_i=j|\vec{x}_i)z_j^2 \ \ - \ \ \left(\sum_{j=1}^2P(Y_i=j|\vec{x}_i)z_j\right)^2  \end{aligned}\end{split}\]</div>
<p>Now, by <strong>Cauchy-Schwarz Inequality</strong>,</p>
<div class="math notranslate nohighlight">
\[\left(\sum_{j=1}^l u_jv_j\right)^2 \ \ \leq \ \ \sum_{j=1}^l u_j^2.\sum_{j=1}^lv_j^2\]</div>
<p>If we set, l =2, <span class="math notranslate nohighlight">\(u_j=\sqrt{P(Y_i=j|\vec{x}_i)}\)</span> also as, <span class="math notranslate nohighlight">\(P(Y_i=j|\vec{x}_i) \geq 0\)</span>, we can set <span class="math notranslate nohighlight">\(v_j=\sqrt{P(Y_i=j|\vec{x}_i)}.z_j\)</span>; we get,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 &amp;\left(\sum_{j=1}^2 \sqrt{P(Y_i=j|\vec{x}_i)}.\sqrt{P(Y_i=j|\vec{x}_i)}.z_j\right)^2 \leq \sum_{j=1}^2P(Y_i=j|\vec{x}_i).\sum_{j=1}^2P(Y_i=j|\vec{x}_i).z_j^2 \\
 \implies &amp; \left(\sum_{j=1}^2 P(Y_i=j|\vec{x}_i).z_j\right)^2 \leq \sum_{j=1}^2P(Y_i=j|\vec{x}_i).z_j^2
\end{aligned}\end{split}\]</div>
<p>As, <span class="math notranslate nohighlight">\(\sum_{j=1}^2P(Y_i=j|\vec{x}_i) = 1\)</span>.</p>
<p>So, <span class="math notranslate nohighlight">\(\vec{z}^T.P.\vec{z} \geq 0\)</span>. So, P is positive semi-definite.</p>
<p>Using the above steps we can prove that -H is negative semi-definite, so H is positive semi-definite. Which in term proves that, if we solve the equation <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \vec{\beta}}log L = 0\)</span> we will always get one solution and the solution will be point of maxima.</p>
<section id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline"></a></h2>
<p>In this article we have seen mainly theory behind Multionomial Logistic Regression including, choosing right distribution for the problem, build the set equations to get the parameter estimates using MLE also proved concavity of the problem. Here we are not actually solving the set of equations of MLE, the reason is, we cannot get closed form solution from the equations, we have to use numerical methods. As we have not discussed any of it we are leaving it for future articles.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2021_10_29_LogisticRegression.html" class="btn btn-neutral float-left" title="Mathematics behind Logistic Regression (Binomial)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Working_With_Links.html" class="btn btn-neutral float-right" title="Workin with Links" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Kaustav.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>